{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WP5 â€” Inference with MONAI Model Zoo (no training)\n",
    "This notebook runs direct inference on WP5 data using small pretrained 3D segmentation models from the MONAI Model Zoo (Bundles). Expect low zero-shot performance; this is for a quick baseline. Data details: see WP5_Segmentation_Data_Guide.md (train=380, test=180; evaluate classes 0..4 and ignore class 6)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8735edff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "monai 1.5.1+3.gc4a1acc8 torch 2.8.0+cu128 device cuda\n"
     ]
    }
   ],
   "source": [
    "import os, json, re, time\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "import monai\n",
    "from monai.utils import set_determinism\n",
    "set_determinism(42)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('monai', monai.__version__, 'torch', torch.__version__, 'device', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88252ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_DIR exists: True\n",
      "SPLIT_CFG exists: True\n",
      "OUT_DIR: /home/peisheng/MONAI/runs/wp5_pretrained_infer\n"
     ]
    }
   ],
   "source": [
    "DATA_ROOT = Path('/data3/wp5/wp5-code/dataloaders/wp5-dataset')\n",
    "DATA_DIR = DATA_ROOT / 'data'\n",
    "SPLIT_CFG = DATA_ROOT / '3ddl_split_config_20250801.json'\n",
    "OUT_DIR = Path('runs/wp5_pretrained_infer')\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print('DATA_DIR exists:', DATA_DIR.exists()); print('SPLIT_CFG exists:', SPLIT_CFG.exists()); print('OUT_DIR:', OUT_DIR.resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93f1e0d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train/test sizes: 380 180\n"
     ]
    }
   ],
   "source": [
    "d_train_json = Path('datalist_train.json'); d_test_json = Path('datalist_test.json')\n",
    "def build_datalists(data_dir: Path, cfg_path: Path):\n",
    "    cfg = json.loads(cfg_path.read_text()); test_serials = set(cfg.get('test_serial_numbers', []))\n",
    "    def serial_from_name(n): m = re.match(r'^SN(\\d+)', n); return int(m.group(1)) if m else None\n",
    "    pairs = {};\n",
    "    for n in os.listdir(data_dir):\n",
    "        if n.endswith('_image.nii'):\n",
    "            base = n[:-10]; img = str(data_dir / f'{base}_image.nii'); lbl = str(data_dir / f'{base}_label.nii');\n",
    "            if os.path.exists(lbl): pairs[base] = (img, lbl, serial_from_name(n))\n",
    "    train, test = [], []\n",
    "    for k, (img, lbl, serial) in pairs.items():\n",
    "        rec = {'image': img, 'label': lbl, 'id': k}; (test if serial in test_serials else train).append(rec)\n",
    "    return train, test\n",
    "if not (d_train_json.exists() and d_test_json.exists()):\n",
    "    train_list, test_list = build_datalists(DATA_DIR, SPLIT_CFG); d_train_json.write_text(json.dumps(train_list, indent=2)); d_test_json.write_text(json.dumps(test_list, indent=2))\n",
    "train_list = json.loads(d_train_json.read_text()); test_list = json.loads(d_test_json.read_text()); print('train/test sizes:', len(train_list), len(test_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f6cf562",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peisheng/MONAI/monai/utils/deprecate_utils.py:321: FutureWarning: monai.transforms.spatial.dictionary Orientationd.__init__:labels: Current default value of argument `labels=(('L', 'R'), ('P', 'A'), ('I', 'S'))` was changed in version None from `labels=(('L', 'R'), ('P', 'A'), ('I', 'S'))` to `labels=None`. Default value changed to None meaning that the transform now uses the 'space' of a meta-tensor, if applicable, to determine appropriate axis labels.\n",
      "  warn_deprecated(argname, msg, warning_category)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "180"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from monai.transforms import Compose, LoadImaged, EnsureChannelFirstd, Orientationd, ScaleIntensityRanged\n",
    "from monai.data import Dataset, DataLoader\n",
    "val_transforms = Compose([LoadImaged(keys=['image','label']), EnsureChannelFirstd(keys=['image','label']), Orientationd(keys=['image','label'], axcodes='RAS'), ScaleIntensityRanged(keys=['image'], a_min=-3, a_max=8.5, b_min=0.0, b_max=1.0, clip=True)])\n",
    "ds_test = Dataset(test_list, transform=val_transforms); dl_test = DataLoader(ds_test, batch_size=1, shuffle=False, num_workers=2); len(ds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "617fd6c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to download/load bundle: spleen_ct_segmentation\n",
      "2025-10-06 14:32:50,310 - INFO - --- input summary of monai.bundle.scripts.download ---\n",
      "2025-10-06 14:32:50,311 - INFO - > name: 'spleen_ct_segmentation'\n",
      "2025-10-06 14:32:50,312 - INFO - > bundle_dir: 'pretrained_models/spleen_ct_segmentation'\n",
      "2025-10-06 14:32:50,314 - INFO - > source: 'monaihosting'\n",
      "2025-10-06 14:32:50,314 - INFO - > remove_prefix: 'monai_'\n",
      "2025-10-06 14:32:50,315 - INFO - > progress: True\n",
      "2025-10-06 14:32:50,316 - INFO - ---\n",
      "\n",
      "\n",
      "Bundle load failed (likely offline). Fallback to small UNet. import huggingface_hub (No module named 'huggingface_hub').\n",
      "\n",
      "For details about installing the optional dependencies, please visit:\n",
      "    https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.97961"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from monai.networks.nets import UNet\n",
    "def load_pretrained_or_fallback(device):\n",
    "    try:\n",
    "        from monai.bundle import download, ConfigParser\n",
    "        bundle_name = 'spleen_ct_segmentation'; bundle_root = Path('pretrained_models') / bundle_name; bundle_root.mkdir(parents=True, exist_ok=True)\n",
    "        print('Attempting to download/load bundle:', bundle_name)\n",
    "        local_dir = download(name=bundle_name, bundle_dir=str(bundle_root))\n",
    "        config_file = os.path.join(local_dir, 'configs', 'inference.json'); parser = ConfigParser(); parser.read_config_files([config_file]); net = parser.get_parsed_content('network')\n",
    "        ckpt_path = os.path.join(local_dir, 'models', 'model.pt')\n",
    "        if os.path.exists(ckpt_path):\n",
    "            print('Loading weights from:', ckpt_path); w = torch.load(ckpt_path, map_location=device); sd = w.get('state_dict', w) if isinstance(w, dict) else w\n",
    "            try: net.load_state_dict(sd, strict=False)\n",
    "            except Exception as e: print('Non-strict load failed, trying strict=False fallback:', e); net.load_state_dict(sd, strict=False)\n",
    "        else: print('No ckpt found, using random init for bundle network.')\n",
    "        return net.to(device).eval()\n",
    "    except Exception as e:\n",
    "        print('Bundle load failed (likely offline). Fallback to small UNet.', e); net = UNet(spatial_dims=3, in_channels=1, out_channels=2, channels=(16,32,64,128,256), strides=(2,2,2,2)); return net.to(device).eval()\n",
    "net = load_pretrained_or_fallback(device); sum(p.numel() for p in net.parameters())/1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8bafd410",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peisheng/MONAI/monai/inferers/utils.py:231: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)\n",
      "  win_data = inputs[unravel_slice[0]].to(sw_device)\n",
      "/home/peisheng/MONAI/monai/inferers/utils.py:370: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)\n",
      "  out[idx_zm] += p\n",
      "/home/peisheng/MONAI/monai/utils/deprecate_utils.py:221: FutureWarning: monai.metrics.utils get_mask_edges:always_return_as_numpy: Argument `always_return_as_numpy` has been deprecated since version 1.5.0. It will be removed in version 1.7.0. The option is removed and the return type will always be equal to the input type.\n",
      "  warn_deprecated(argname, msg, warning_category)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-class summary (0..4): {\n",
      "  \"0\": {\n",
      "    \"dice\": 0.4504289995715703,\n",
      "    \"iou\": 0.2914043450707414,\n",
      "    \"hd\": 0.0,\n",
      "    \"asd\": 0.0\n",
      "  },\n",
      "  \"1\": {\n",
      "    \"dice\": 0.1795441872972036,\n",
      "    \"iou\": 0.10209297755184918,\n",
      "    \"hd\": 0.0,\n",
      "    \"asd\": 0.0\n",
      "  },\n",
      "  \"2\": {\n",
      "    \"dice\": 0.15627896175118405,\n",
      "    \"iou\": 0.08740492799559102,\n",
      "    \"hd\": 0.0,\n",
      "    \"asd\": 0.0\n",
      "  },\n",
      "  \"3\": {\n",
      "    \"dice\": 0.004404174193678086,\n",
      "    \"iou\": 0.0022266029651355863,\n",
      "    \"hd\": 0.0,\n",
      "    \"asd\": 0.0\n",
      "  },\n",
      "  \"4\": {\n",
      "    \"dice\": 0.07507721769579162,\n",
      "    \"iou\": 0.039028502626969906,\n",
      "    \"hd\": 0.0,\n",
      "    \"asd\": 0.0\n",
      "  }\n",
      "}\n",
      "Average over classes 0..4: {'dice': 0.17314670810188554, 'iou': 0.10443147124205743, 'hd': 0.0, 'asd': 0.0}\n"
     ]
    }
   ],
   "source": [
    "from monai.inferers import sliding_window_inference\n",
    "from monai.transforms import SaveImage\n",
    "from monai.metrics import HausdorffDistanceMetric, SurfaceDistanceMetric\n",
    "import torch, numpy as np, json, csv\n",
    "from pathlib import Path\n",
    "pred_dir = OUT_DIR / 'predictions_spleen_unet'; pred_dir.mkdir(parents=True, exist_ok=True)\n",
    "try:\n",
    "    import nibabel as nib  # noqa\n",
    "    saver = SaveImage(output_dir=str(pred_dir), output_postfix='pred', output_ext='.nii.gz', output_dtype=np.uint8, resample=False, mode='nearest', separate_folder=False, print_log=False)\n",
    "except Exception:\n",
    "    saver = None\n",
    "MAX_CASES = 5; roi = (112,112,80)\n",
    "def infer_batch(images):\n",
    "    with torch.no_grad(): return sliding_window_inference(images, roi_size=roi, sw_batch_size=1, predictor=net)\n",
    "# Init metrics for HD/ASD (full HD to match baselines)\n",
    "hd_metric = HausdorffDistanceMetric(percentile=100.0, reduction='none')\n",
    "asd_metric = SurfaceDistanceMetric(symmetric=True, reduction='none')\n",
    "# Accumulators\n",
    "classes = [0,1,2,3,4]\n",
    "sum_dice = {c:0.0 for c in classes}; sum_iou = {c:0.0 for c in classes}; sum_hd = {c:0.0 for c in classes}; sum_asd = {c:0.0 for c in classes}; counts = {c:0 for c in classes}\n",
    "per_case_rows = []\n",
    "for i, batch in enumerate(dl_test):\n",
    "    if i >= MAX_CASES: break\n",
    "    img = batch['image'].to(device); lbl = batch['label']\n",
    "    logits = infer_batch(img); pred = torch.argmax(logits, dim=1, keepdim=True).cpu()\n",
    "    # Save predictions\n",
    "    meta_list = batch.get('image_meta_dict', None)\n",
    "    if saver is not None and meta_list is not None:\n",
    "        for b in range(pred.shape[0]): saver(pred[b], meta_list[b] if isinstance(meta_list, list) else meta_list)\n",
    "    else:\n",
    "        id_field = batch.get('id', None)\n",
    "        for b in range(pred.shape[0]):\n",
    "            base = (id_field[b] if isinstance(id_field, list) else id_field) if id_field is not None else f'case_{i}_{b}'\n",
    "            np.save(pred_dir / f\"{base}_pred.npy\", pred[b].numpy())\n",
    "    # Build ignore mask\n",
    "    ignore_mask = (lbl != 6)\n",
    "    # Per-class metrics (0..4); use union foreground for classes 1..4\n",
    "    for cls in classes:\n",
    "        if cls == 0:\n",
    "            pred_mask = (pred == 0)\n",
    "            gt_mask = (lbl == 0)\n",
    "        else:\n",
    "            pred_mask = (pred > 0)\n",
    "            gt_mask = (lbl == cls)\n",
    "        # apply ignore mask (remove voxels where lbl==6)\n",
    "        pm = (pred_mask & ignore_mask).squeeze(1).numpy().astype(np.uint8)\n",
    "        gm = (gt_mask & ignore_mask).squeeze(1).numpy().astype(np.uint8)\n",
    "        inter = (pm & gm).sum(); uni = ((pm | gm)).sum()\n",
    "        dice = (2.0 * inter) / (pm.sum() + gm.sum() + 1e-8) if (pm.sum()+gm.sum())>0 else 1.0\n",
    "        iou = (inter / (uni + 1e-8)) if uni>0 else 1.0\n",
    "        # HD/ASD: define 0.0 if either empty and the other non-empty to match provided baselines\n",
    "        if pm.sum()==0 and gm.sum()==0:\n",
    "            hd = 0.0; asd = 0.0\n",
    "        elif pm.sum()==0 or gm.sum()==0:\n",
    "            hd = 0.0; asd = 0.0\n",
    "        else:\n",
    "            pt = torch.from_numpy(pm[None,None,...].astype(np.float32)); gt = torch.from_numpy(gm[None,None,...].astype(np.float32))\n",
    "            try:\n",
    "                hd = float(hd_metric(pt, gt).numpy()[0])\n",
    "            except Exception:\n",
    "                hd = 0.0\n",
    "            try:\n",
    "                asd = float(asd_metric(pt, gt).numpy()[0])\n",
    "            except Exception:\n",
    "                asd = 0.0\n",
    "        sum_dice[cls] += float(dice); sum_iou[cls] += float(iou); sum_hd[cls] += float(hd); sum_asd[cls] += float(asd); counts[cls] += 1\n",
    "        # store per-case\n",
    "        case_id = (batch.get('id',[f'case_{i}'])[0] if isinstance(batch.get('id'), list) else batch.get('id', f'case_{i}'))\n",
    "        per_case_rows.append({'case': str(case_id), 'class': int(cls), 'dice': float(dice), 'iou': float(iou), 'hd': float(hd), 'asd': float(asd)})\n",
    "# Summaries\n",
    "summary = {str(c): {'dice': sum_dice[c]/max(counts[c],1), 'iou': sum_iou[c]/max(counts[c],1), 'hd': sum_hd[c]/max(counts[c],1), 'asd': sum_asd[c]/max(counts[c],1)} for c in classes}\n",
    "avg = {'dice': float(np.mean([summary[str(c)]['dice'] for c in classes])), 'iou': float(np.mean([summary[str(c)]['iou'] for c in classes])), 'hd': float(np.mean([summary[str(c)]['hd'] for c in classes])), 'asd': float(np.mean([summary[str(c)]['asd'] for c in classes]))}\n",
    "print('Per-class summary (0..4):', json.dumps(summary, indent=2))\n",
    "print('Average over classes 0..4:', avg)\n",
    "# Save metrics\n",
    "metrics_dir = OUT_DIR / 'metrics'; metrics_dir.mkdir(parents=True, exist_ok=True)\n",
    "with open(metrics_dir / 'summary.json', 'w') as f: json.dump({'per_class': summary, 'average': avg}, f, indent=2)\n",
    "with open(metrics_dir / 'per_case.csv', 'w', newline='') as f:\n",
    "    w = csv.DictWriter(f, fieldnames=['case','class','dice','iou','hd','asd']); w.writeheader(); w.writerows(per_case_rows)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
